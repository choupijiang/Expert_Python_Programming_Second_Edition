# Chapter 2 － Syntax Best Practices – below the Class Level
The ability to write an ef cient syntax comes naturally with time. If you take a look back at your  rst program, you will probably agree with this. The right syntax will appear to your eyes as a good-looking piece of code, and the wrong syntax
as something disturbing.  

Besides the algorithms that are implemented and the architectural design for your program, taking great care over how it is written weighs heavily on how it will evolve. Many programs are ditched and rewritten from scratch because of their obtuse syntax, unclear APIs, or unconventional standards.  

对你的程序来说，除了算法实现和架构设计之外，

But Python has evolved a lot in the last few years. So, if you were kidnapped for a while by your neighbor (a jealous guy from the local Ruby developers user group) and kept away from the news, you will probably be astonished by its new features. From the earliest version to the current one (3.5 at this time), a lot of enhancements have been made to make the language clearer, cleaner, and easier to write. Python basics have not changed drastically, but the tools to play with them are now a lot more ergonomic.  

在过去的几年中Python又了很多的进化，如果你

This chapter presents the most important elements of modern syntax and tips on their usage:  


- List comprehensions
- Iterators and generators
- Descriptors and properties
- Decorators
- with and contextlib

The code performance tips for speed improvement or memory usage are covered in Chapter 11, Optimization – General Principles and Pro ling Techniques, and Chapter 12, Optimization – Some Powerful Techniques.  

## Python's built-in types Python的内建类型
Python provides a great set of datatypes. This is true for both numeric types and also collections. Regarding the numeric types, there is nothing special about their syntax. There are, of course, some differences for de ning literals of every type and some (maybe) not well-known details regarding operators, but there aren't a lot of choices left for developers. Things change when it comes to collections and strings. Despite the "there should be only one way to do something" mantra, the Python developer is really left with plenty of choices. Some of the code patterns that seem intuitive and simple to beginners are often considered non-Pythonic by experienced programmers because they are either inef cient or simply too verbose.  

Python提供了大量的数据类型集。这对于数字类型和集合类型来说更是如此。

Such `Pythonic` patterns for solving common problems (by many programmers called idioms) may often seem like only aesthetics. This cannot be more wrong. Most of the idioms are driven by the fact how Python is implemented internally and on how built- in structures and modules work. Knowing more of such details is essential for a good understanding of the language. Also, the community itself is not free from myths and stereotypes about how things in Python work. Only by digging deeper yourself, will you be able to tell which of the popular statements about Python are really true.  

### Strings and bytes
The topic of strings may provide some confusion for programmers that are used to programming only in Python 2. In Python 3, there is only one datatype capable of storing textual information. It is str or, simply, string. It is an immutable sequence that stores Unicode code points. This is the major difference from Python 2, where str represents byte strings—something that is now handled by the bytes objects (but not exactly in the same way).  

字符串这个话题让过去只在Python 2中编程的程序员感觉有些困惑。在Python 3中仅有一种能够存储文本信息的数据类型。它就是str，或者简单来说就是字符串。

Strings in Python are sequences. This single fact should be enough to include them in the section covering other container types, but they differ from other container types in one important detail. Strings have very speci c limitations on what type of data they can store, and that is Unicode text.  

Python中的字符串就是序列。

bytes and its mutable alternative (bytearray) differs from str by allowing only bytes asasequence value—integers in the range `0 <= x < 256`. This may be confusing at the beginning, since when printed, they may look very similar to strings:  

```python
>>> print(bytes([102, 111, 111]))
b'foo'
```

The true nature of bytes and bytearray is revealed when it is converted to another sequence type like list or tuple:  

```python
>>> list(b'foo bar')
[102, 111, 111, 32, 98, 97, 114]
>>> tuple(b'foo bar')
(102, 111, 111, 32, 98, 97, 114)
```

A lot of Python 3 controversy was about breaking the backwards compatibility for string literals and how Unicode is dealt with. Starting from Python 3.0, every un- pre xed string literal is Unicode. So, literals enclosed by single quotes ('), double quotes ("), or groups of three quotes (single or double) without any pre x represent the str datatype:  

```python
>>> type("some string")
<class 'str'>
```

In Python 2, the Unicode literals required the u pre x (like u"some string"). This pre x is still allowed for backward compatibility (starting from Python 3.3), but does not hold any syntactic meaning in Python 3.  

Bytes literals were already presented in some of the previous examples, but let's explicitly present its syntax for the sake of consistency. Bytes literals are also enclosed by single quotes, double quotes, or triple quotes, but must be preceded by a b or B pre x:  

```python
>>> type(b"some bytes")
<class 'bytes'>
```

Note that there is no bytearray literals in the Python syntax.  

注意在Python语法中并不存在bytearray字面量。  

Last but not least, Unicode strings contain "abstract" text that is independent from the byte representation. This makes them unable to be saved on the disk or sent over the network without encoding to binary data. There are two ways to encode string objects into byte sequences:  

- Using the str.encode(encoding, errors) method, which encodes the string using a registered codec for encoding. Codec is specified using the encoding argument, and, by default, it is 'utf-8'. The second errors argument specifies the error handling scheme. It can be 'strict' (default), 'ignore', 'replace', 'xmlcharrefreplace', or any other registered handler (refer to the built-in codecs module documentation).

- Using the bytes(source, encoding, errors) constructor, which creates a new bytes sequence. When the source is of the str type, then the encoding argument is obligatory and it does not have a default value. The usage of
the encoding and errors arguments is the same as for the str.encode() method.  

Binary data represented by bytes can be converted to a string in the analogous ways:  

- Using the bytes.decode(encoding, errors) method, which decodes the bytes using the codec registered for encoding. The arguments of this method have the same meaning and defaults as the arguments of str.encode().
- Using the str(source, encoding, error) constructor, which creates a new string instance. Similar to the bytes() constructor, the encoding argument in the str() call has no default value and must be provided if the bytes sequence is used as a source.

>#### Tip
>Naming – bytes versus byte string
>Due to changes made in Python 3, some people tend to refer to the bytes instances as byte strings. This is mostly due to historic reasons—bytes in Python 3 is the sequence type that is the closest one to the str type from Python 2 (but not the same). Still, the bytes instance is a sequence of bytes and also does not need to represent textual data. So, in order to avoid any confusion, it is advisable to always refer to them as either bytes or a byte sequence despite their similarities to strings. The concept of strings is reserved for textual data in Python 3 and this is now always str.

#### Implementation details
Python strings are immutable. This is also true to byte sequences. This is an important fact because it has both advantages and disadvantages. It also affects the way strings should be handled in Python ef ciently. Thanks to immutability, strings can be used as dictionary keys or set collection elements because once initialized, they will never change their value. On the other hand, whenever a modi ed string is required (even with only tiny modi cation), a completely new instance needs to be created. Fortunately, bytearray as a mutable version of bytes does not introduce such an issue. Byte arrays can be modi ed in-place (without the need of new object creation) through item assignments and can be dynamically resized exactly like lists—using appends, pops, inserts, and so on.  

Python字符串是可变的。对于字节序列也是同样。这是一个重要的事实，因为它的优缺点并存。

#### String concatenation 字符串的连接
Knowing the fact that Python strings are immutable imposes some problems when multiple string instances need to be joined together. As stated before, concatenating any immutable sequences result in the creation of a new sequence object. Consider that a new string is built by the repeated concatenation of multiple strings, as follows:  

```python
   s = ""
   for substring in substrings:
       s += substring
```

This will result in a quadratic runtime cost in the total string length. In other words, it is highly inef cient. For handling such situations, there is the str.join() method available. It accepts iterable of strings as the argument and returns a joined string. Because it is the method, the actual idiom uses the empty string literal as a source
of method:  

```python
   s = "".join(substrings)
```

The string providing this method will be used as a separator between joined substrings; consider the following example:  

```python
>>> ','.join(['some', 'comma', 'separated', 'values'])
'some,comma,separated,values'
```

It is worth remembering that just because it is faster (especially for large lists), it does not mean that the join() method should be used in every situation where two strings need to be concatenated. Despite being a widely recognized idiom, it does not improve code readability – and readability counts! There are also some situations where join() may not perform as well as ordinary concatenation through addition. Here some examples of them:  

它值得你记住是因为它的快（特别是对于大的列表），但这也并意味着join()方法可以用于任何需要连接两个字符串的地方。

- If the number of substrings is small and they are not contained already by some iterable—in some cases, an overhead of creating a new sequence just to perform concatenation can overshadow the gain of using join().
- When concatenating short literals, thanks to constant folding in CPython, some complex literals (not only strings) such as 'a' + 'b' + 'c' to 'abc' can be translated to a shorter form at compile time. Of course, this is enabled only for constants (literals) that are relatively short.

Ultimately, the best readability of string concatenation if the number of strings is known beforehand is ensured by proper string formatting, by either using the str. format() method or the % operator. In code sections where the performance is not critical or gain from optimizing string concatenation is very little, string formatting is recommended as the best alternative.  

>#### Tip Constant folding and peephole optimizer
>CPython uses the peephole optimizer on compiled source code in order to improve performance. This optimizer implements a number of common optimizations directly on Python's byte code. As mentioned, constant folding is one such feature. The resulting constants are limited in length by a hardcoded value. In Python 3.5, it is still invariably equal to 20. Anyway, this particular detail is rather a curiosity than a thing that can be relied on in day-to-day programming. Information of other interesting optimizations performed by peephole optimizer can be found in the Python/peephole.c  le of Python's source code. 


### Collections 群集
Python provides a good selection of built-in data collections that allows you to ef ciently solve many problems if you choose wisely. Types that you probably already know are those that have dedicated literals:  

如果你选择没错的话，Python提供了允许你更有效率解决很多问题的选择。

- Lists
- Tuples
- Dictionaries 
- Sets

Python is of course not limited to these four and it extends the list of possible choices through its standard library. In many cases, the solution to a problem may be as simple as making a good choice for data structure. This part of the book aims to ease such a decision by providing deeper insight into the possible options.  

#### Lists and tuples
The two most basic collection types in Python are lists and tuples, and they both represent sequences of objects. The basic difference between them should be obvious for anyone who has spent more than a few hours with Python—lists are dynamic so can change their size, while tuples are immutable (they cannot be modi ed after they are created).  

Python中的两个最基础的群集类型为列表和元组，

Tuples, despite having many various optimizations that makes allocation/ deallocation of small objects fast, are the recommended datatype for structures where the position of the element is information by itself. For example, tuple may be a good choice for storing a pair of (x, y) coordinates. Anyway, details regarding tuples are rather uninteresting. The only important thing about them in the scope of this chapter is that tuple is **immutable** and thus **hashable**. What this means will be covered later in a Dictionaries section. More interesting than tuple is its dynamic counterpart, list, how it really works, and how to deal with it ef ciently.  

#### Implementation details 实现细节
Many programmers easily confuse Python's list type with the concept of linked lists found often in standard libraries of other languages such as C, C++, or Java. In fact, CPython lists are not lists at all. In CPython, lists are implemented as variable length arrays. This should also be true for other implementations such as Jython and IronPython, although such implementation details are often not documented in these projects. The reasons for such confusion are clear. This datatype is named list and also has an interface that could be expected from any linked list implementation.  

很多的程序员

Why is it important and what does it mean? Lists are one of the most popular data structures and the way they are used greatly affects every application's performance. Also, CPython is the most popular and used implementation, so knowing its internal implementation details is crucial.  

为什么它如此重要，而且它的意义何在？列表是最流行数据结构中的一个，

In detail, lists in Python is a contiguous array of references to other objects. The pointer to this array and the length is stored in a lists head structure. This means that every time an item is added or removed, the array of references needs to be resized (reallocated). Fortunately, in Python, these arrays are created with exponential over-allocation, so not every operation requires a resize. This is how the amortized cost of appending and popping elements can be low in terms of complexity. Unfortunately, some other operations that are considered "cheap" in ordinary
linked lists have relatively high computational complexity in Python:  

具体来说，

- Inserting an item at arbitrary place using the list.insert method— complexity O(n)
- Deleting an item using list.delete or using del—complexity O(n)

Here, n is the length of a list. At least retrieving or setting an element using index is an operation that cost is independent of the list's size. Here is a full table of average time complexities for most of the list operations:  

表格：省略  

For situations where a real linked list is needed (or simply, a data structure that has appends and pop at each side at O(1) complexity), Python provides deque in collections built-in module. This is a generalization of stacks and queues and should work  ne anywhere where a doubly linked list is required.  

##### List comprehensions 列表解析式
As you probably know, writing a piece of code such as this is painful:  

你可能又知道，写出这样一段代码是很令人头疼的：  

```python
>>> evens = []
>>> for i in range(10):
...     if i % 2 == 0:
...         evens.append(i)
...
>>> evens
[0, 2, 4, 6, 8]
```

This may work for C, but it actually makes things slower for Python because:  

对C来说这是可以正常工作的，但实际上对于Python来说却很慢，因为：  

- It makes the interpreter work on each loop to determine what part of the sequence has to be changed
- It makes you keep a counter to track what element has to be treated
- It requires an additional function lookup to be performed at every iteration because append() is a list's method

- 

A list comprehension is the correct answer to this pattern. It uses wired features that
automate parts of the previous syntax:  

```python
>>> [i for i in range(10) if i % 2 == 0]
[0, 2, 4, 6, 8]
```

Besides the fact that this writing is more ef cient, it is way shorter and involves fewer elements. In a bigger program, this means fewer bugs and code that is easier to read and understand.  

>Tip
>List comprehensions and internal array resize

>There is a myth among some Python programmers that the list comprehensions can be a workaround for the fact that the internal array representing the list object must be resized with every few additions. Some say that the array will be allocated once in just the right size. Unfortunately, this isn't true.

>The interpreter during evaluation of the comprehension can't know how big the resulting container will be and it can't preallocate the final size of the array for it. Due to this, the internal array is reallocated in the same pattern as it would be in the for loop. Still, in many cases, list creation using comprehensions is both cleaner and faster than using ordinary loops.

##### Other idioms

Another typical example of a Python idiom is the usage of enumerate. This built-in function provides a convenient way to get an index when a sequence is used in a loop. Consider the following piece of code as an example:  

```
>>> i = 0
>>> for element in ['one', 'two', 'three']:
...     print(i, element)
...     i += 1
...
0 one
1 two
2 three
```

This can be replaced by the following code, which is shorter:  

```
>>> for i, element in enumerate(['one', 'two', 'three']):
...     print(i, element)
...
0 one
1 two
2 three
```

When the elements of multiple lists (or any iterables) need to be aggregated in a one-by-one fashion, then the built-in zip() function may be used. This is a very common pattern for uniform iteration over two same-sized iterables:  

```
>>> for item in zip([1, 2, 3], [4, 5, 6]):
...     print(item)
...     
(1, 4)
(2, 5)
(3, 6)
```

Note that the results of zip() can be reversed by another zip() call:  

```
>>> for item in zip(*zip([1, 2, 3], [4, 5, 6])):
...     print(item)
...     
(1, 2, 3)
(4, 5, 6)
```

Another popular syntax element is sequence unpacking. It is not limited to lists and tuples and will work with any sequence type (even strings and byte sequences). It allows you to unpack a sequence of elements into another set of variables as long as there are as many variables on the left-hand side of the assignment operator as the number of elements in the sequence:  

```
>>> first, second, third = "foo", "bar", 100
>>> first
'foo'
>>> second
'bar'
>>> third
100
```

Unpacking also allows you to capture multiple elements in a single variable using starred expressions as long as it can be interpreted unambiguously. Unpacking can also be performed on nested sequences. This can come in handy especially when iterating on some complex data structures built of sequences. Here are some examples of more complex unpacking:  

```
>>> # starred expression to capture rest of the sequence
>>> first, second, *rest = 0, 1, 2, 3
>>> first
0
>>> second
1
>>> rest
[2, 3]

>>> # starred expression to capture middle of the sequence
>>> first, *inner, last = 0, 1, 2, 3
>>> first
0
>>> inner
[1, 2]
>>> last
3

>>> # nested unpacking
>>> (a, b), (c, d) = (1, 2), (3, 4)
>>> a, b, c, d
(1, 2, 3, 4)
```

#### Dictionaries
Dictionaries are one of the most versatile data structures in Python. dict allows to map a set of unique keys to values as follows:  

```
{
    1: ' one',
    2: ' two',
    3: ' three',
}
```

Dictionary literals are a very basic thing and you should already know them. Anyway, Python allows programmers to also create a new dictionary using comprehensions similar to the list comprehensions mentioned earlier. Here is a very simple example:  

```python
squares = {number: number**2 for number in range(100)}
```

What is important is that the same benefits of using list comprehensions apply to dictionary comprehensions. So, in many cases, they are more efficient, shorter, and cleaner. For more complex code, when many if statements or function calls are required to create a dictionary, the simple for loop may be a better choice, especially if it improves the readability.  

For Python programmers new to Python 3, there is one important note about iterating over dictionary elements. The dictionary methods: keys(), values(), and items() no longer have lists as their return value types. Also, their counterparts iterkeys(), itervalues(), and iteritems() that returned iterators instead are missing in Python 3. Instead, what keys(), values(), and items() return now are view objects:  

keys(): This returns the dict_keys object that provides a view on all the keys of a dictionary
values(): This returns the dict_values object that provides views on all the values of a dictionary
items(): This returns the dict_items object providing views on all (key, value) two tuples of a dictionary
View objects provide a view on the dictionary content in a dynamic way, so every time the dictionary changes, the views will reflect these changes, as shown in this example:  

```shell
>>> words = {'foo': 'bar', 'fizz': 'bazz'}
>>> items = words.items()
>>> words['spam'] = 'eggs'
>>> items
dict_items([('spam', 'eggs'), ('fizz', 'bazz'), ('foo', 'bar')])
```

View objects join the behavior of lists returned by implementation of old methods with iterators returned by their "iter" counterparts. Views do not need to redundantly store all values in memory (like lists do), but still allow getting their length (using len) and testing membership (using the in clause). Views are, of course, iterable.  

The last important thing is that both views returned by the keys() and values() methods ensure the same order of keys and values. In Python 2, you could not modify the dictionary content between these two calls if you wanted to ensure the same order of retrieved keys and values. dict_keys and dict_values are now dynamic so even if the content of a dictionary will change between keys() and values() calls, the order of iteration is consistent between these two views.  

##### Implementation details

CPython uses hash tables with pseudo-random probing as an underlying data structure for dictionaries. It seems like a very deep implementation detail, but it is very unlikely to change in the near future, so it is also a very interesting fact for the programmer.  

CPython使用

Due to this implementation detail, only objects that are hashable can be used as a dictionary key. An object is hashable if it has a hash value that never changes during its lifetime and can be compared to different objects. Every Python's built-in type that is immutable is also hashable. Mutable types such as list, dictionaries, and sets are not hashable and so they cannot be used as dictionary keys. Protocol that defines if a type is hashable consists of two methods:  

- **__hash__**: This provides the hash value (as an integer) that is needed by the internal dict implementation. For objects that are instances of user-defined classes, it is derived from their id().
- **__eq__**: This compares if two objects that have the same value. All objects that are instances of user-defined classes compare unequal, by default, except for themselves.

Two objects that are compared equal must have the same hash value. The reverse does not need to be true. This means collisions of hashes are possible—two objects with the same hash may not be equal. It is allowed, and every Python implementation must be able to resolve hash collisions. CPython uses open addressing to resolve such collisions (https://en.wikipedia.org/wiki/Open_addressing). Still, the probability of collisions greatly affects performance, and if it is high, the dictionary will not benefit from its internal optimizations.  

While three basic operations: adding, getting, and deleting an item have an average time complexity equal to O(1), their amortized worst case complexities are a lot higher—O(n), where n is the current dictionary size. Additionally, if user-defined class objects are used as dictionary keys and they are hashed improperly (with a high risk of collisions), then this will have a huge negative impact on the dictionary performance. The full table of CPyhton's time complexities for dictionaries is as follows:  

table:omit  

It is also important to know that the n number in worst-case complexities for copying and iterating the dictionary is the maximum size that the dictionary ever achieved, rather than the current item count. In other words, iterating over the dictionary that once was huge but has greatly shrunk in time may take a surprisingly long time. So, in some cases, it may be better to create a new dictionary object if it has to be iterated often instead of just removing elements from the previous one.  

##### Weaknesses and alternatives 缺点和选择

One of the common pitfalls of using dictionaries is that they do not preserve the order of elements in which new keys were added. In some scenarios, when dictionary keys use consecutive keys whose hashes are also consecutive values (for example, using integers), the resulting order might be the same due to the internal implementation of dictionaries:  

使用字典的其中一个陷阱是它们不保留新加入键的顺序。在某些场景中，

```python
>>> {number: None for number in range(5)}.keys()
dict_keys([0, 1, 2, 3, 4])
```

Still, using other datatypes which hash differently shows that the order is not preserved. Here is an example in CPython:  

```python
>>> {str(number): None for number in range(5)}.keys()
dict_keys(['1', '2', '4', '0', '3'])
>>> {str(number): None for number in reversed(range(5))}.keys()
dict_keys(['2', '3', '1', '4', '0'])
```

As shown in the preceding code, the resulting order is both dependent on the hashing of the object and also on the order in which the elements were added. This is not what can be relied on because it can vary with different Python implementations.  

Still, in some cases, the developer might need dictionaries that preserve the order of additions. Fortunately, the Python standard library provides an ordered dictionary called OrderedDict in the collections module. It optionally accepts an iterable as the initialization argument:  

```python
>>> from collections import OrderedDict
>>> OrderedDict((str(number), None) for number in range(5)).keys()
odict_keys(['0', '1', '2', '3', '4'])
```

It also has some additional features such as popping items from both ends using the popitem() method or moving the specified element to one of the ends using the move_to_end() method. A full reference on that collection is available in the Python documentation (refer to https://docs.python.org/3/library/collections.html).  

The other important note is that in very old code bases, dict may be used as a primitive set implementation that ensures the uniqueness of elements. While this will give proper results, this should be omitted unless Python versions lower than 2.3 are targeted. Using dictionaries this way is wasteful in terms of resources. Python has a built-in set type that serves this purpose. In fact, it has a very similar internal implementation to dictionaries in CPython, but offers some additional features as well as specific set-related optimizations.  

#### Sets
Sets are a very robust data structure that are useful mostly in situations where the order of elements is not as important as their uniqueness and efficiency of testing if an element is contained by a collection. They are very similar to analogous mathematic concepts. Sets are provided as built-in types in two flavors:  

- set(): This is a mutable, non-ordered, finite collection of unique, immutable (hashable) objects
- frozenset(): This is an immutable, hashable, non-ordered collection of unique, immutable (hashable) objects

The immutability of frozenset() makes it possible to be used as dictionary keys and also other set() and frozenset() elements. A plain mutable set() cannot be used within another set or frozenset content as this will raise TypeError:  

```shell
>>> set([set([1,2,3]), set([2,3,4])])
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: unhashable type: 'set'
```

The following set initializations are completely correct:  

```shell
>>> set([frozenset([1,2,3]), frozenset([2,3,4])])
{frozenset({1, 2, 3}), frozenset({2, 3, 4})}
>>> frozenset([frozenset([1,2,3]), frozenset([2,3,4])])
frozenset({frozenset({1, 2, 3}), frozenset({2, 3, 4})})
```

Mutable sets can be created in three ways:  

- Using a set() call that accepts optional iterable as the initialization argument, such as set([0, 1, 2])
- Using a set comprehension such as {element for element in range(3)}
- Using set literals such as {1, 2, 3}

Note that using literals and comprehensions for sets requires extra caution because they are very similar in form to dictionary literals and comprehensions. Also, there is no literal for empty set objects—empty curly brackets {} are reserved for empty dictionary literals.

##### Implementation details

Sets in CPython are very similar to dictionaries. As a matter of fact, they are implemented like dictionaries with dummy values, where only keys are actual collection elements. Also, sets exploit this lack of values in mapping for additional optimizations.  

Thanks to this, sets allow very fast additions, deletions, and checking for element existence with the average time complexity equal to O(1). Still, since the implementation of sets in CPython relies on a similar hash table structure, the worst-case complexity for these operations is O(n), where n is the current size of a set.  

Other implementation details also apply. The item to be included in a set must be hashable, and if instances of user-defined classes in a set are hashed poorly, this will have a negative impact on the performance.  

#### Beyond basic collections – the collections module
Every data structure has its shortcomings. There is no single collection that can suit every problem and four basic types of them (tuple, list, set, and dictionary) is still not a wide range of choices. These are the most basic and important collections that have a dedicated literal syntax. Fortunately, Python provides a lot more options in its standard library through the collections built-in module. One of them was already mentioned (deque). Here are the most important collections provided by this module:  

- namedtuple(): This is a factory function for creating tuple subclasses whose indexes can be accessed as named attributes
- deque: This is a double-ended queue, list-like generalization of stacks and queues with fast appends and pops on both ends
- ChainMap: This is a dictionary-like class to create a single view of multiple mappings
- Counter: This is a dictionary subclass for counting hashable objects
- OrderedDict: This is a dictionary subclass that preserves the order the entries were added in
- defaultdict: This is a dictionary subclass that can supply missing values with a provided default

>Note
>More details on selected collections from the collections module and some advice on where it is worth using them are provided in Chapter 12, Optimization – Some Powerful Techniques.

### Advanced syntax 高级语法

It is hard to objectively tell which element of language syntax is advanced. For the purpose of this chapter on advanced syntax elements, we will consider the elements that do not directly relate to any specific built-in datatypes and which are relatively hard to grasp at the beginning. The most common Python features that may be hard to understand are:  

很难客观地来语言的哪一个基础语法是高级的。

- Iterators
- Generators
- Decorators
- Context managers

#### Iterators 迭代器

An **iterator** is nothing more than a container object that implements the iterator protocol. It is based on two methods:  

迭代器除仅仅是一个实现了迭代器协议的容器对象。它基于两个方法：  

- **__next__**: This returns the next item of the container
- **__iter__**: This returns the iterator itself

- **__next__**：该方法返回容器的下一个项
- **__iter__**：该方法返回迭代器本身

Iterators can be created from a sequence using the iter built-in function. Consider the following example:  

迭代器能够使用内建函数iter从一个序列中创建。考虑下面的例子：  

```shell
>>> i = iter('abc')
>>> next(i)
'a'
>>> next(i)
'b'
>>> next(i)
'c'
>>> next(i)
Traceback (most recent call last):
  File "<input>", line 1, in <module>
StopIteration
```

When the sequence is exhausted, a StopIteration exception is raised. It makes iterators compatible with loops since they catch this exception to stop cycling. To create a custom iterator, a class with a __next__ method can be written, as long as it provides the special method __iter__ that returns an instance of the iterator:  

当序列耗尽之时，异常StopIteration便会抛出。这能够让迭代器和循环兼容，因为它们捕捉这个异常以停止循环。要创建一个自定义的迭代器，

```python
class CountDown:
    def __init__(self, step):
        self.step = step

    def __next__(self):
        """
        Return the next element.
        返回下一个元素
        """
        if self.step <= 0:
            raise StopIteration
        self.step -= 1
        return self.step

    def __iter__(self):
        """
        Return the iterator itself.
        返回迭代器自身
        """
        return self
```

Here is example usage of such iterator:  

这里是此类迭代器中用例：  

```
>>> for element in CountDown(4):
...     print(element)
...     
3
2
1
0
```

Iterators themselves are a low-level feature and concept, and a program can live without them. But they provide the base for a much more interesting feature, generators.  

### The yield statement

Generators provide an elegant way to write simple and efficient code for functions that return a sequence of elements. Based on the yield statement, they allow you to pause a function and return an intermediate result. The function saves its execution context and can be resumed later, if necessary.   

生成器提供了一种更优雅的方式来写出简单有效的返回一系列元素的函数代码。基于yield语句，你可以终止一个函数，并返回一个中间过渡结果。如果有必要的话，函数可以保存自身的执行上下文并在稍后使用。  

For instance, the Fibonacci series can be written with an iterator (this is the example provided in the PEP about iterators):  

例如，Fibonacci序列用一个迭代器编写出来（）

```python
def fibonacci():
    a, b = 0, 1
    while True:
        yield b
        a, b = b, a + b
```

You can retrieve new values from generators as if it were iterators, so using next() function or for loops:  

如果返回值是迭代器的话，你可以从生成器重新取回值，所以使用next()函数或者for循环：  

```
>>> fib = fibonacci()
>>> next(fib)
1
>>> next(fib)
1
>>> next(fib)
2
>>> [next(fib) for i in range(10)]
[3, 5, 8, 13, 21, 34, 55, 89, 144, 233]
```

This function returns a generator object, a special iterator, which knows how to save the execution context. It can be called indefinitely, yielding the next element of the suite each time. The syntax is concise, and the infinite nature of the algorithm does not disturb the readability of the code anymore. It does not have to provide a way to make the function stoppable. In fact, it looks similar to how the series would be designed in pseudocode.  

该函数返回一个生成器对象，

In the community, generators are not used so often because the developers are not used to thinking this way. The developers have been used to working with straight functions for years. Generators should be considered every time you deal with a function that returns a sequence or works in a loop. Returning the elements one at a time can improve the overall performance, when they are passed to another function for further work.  

在社区中，生成器并不是经常使用，因为开发者

In that case, the resources used to work out one element are most of the time less important than the resources used for the whole process. Therefore, they can be kept low, making the program more efficient. For instance, the Fibonacci sequence is infinite, and yet the generator that generates it does not require an infinite amount of memory to provide the values one at a time. A common use case is to stream data buffers with generators. They can be paused, resumed, and stopped by third-party code that plays over the data, and all the data does not need to be loaded before starting the process.  

在这种情况之下，

The tokenize module from the standard library, for instance, generates tokens out of a stream of text and returns an iterator for each treated line that can be passed along to some processing:  

例如，来自标准库的tokenize模块，

```
>>> import tokenize
>>> reader = open('hello.py').readline
>>> tokens = tokenize.generate_tokens(reader)
>>> next(tokens)
TokenInfo(type=57 (COMMENT), string='# -*- coding: utf-8 -*-', start=(1, 0), end=(1, 23), line='# -*- coding: utf-8 -*-\n')
>>> next(tokens)
TokenInfo(type=58 (NL), string='\n', start=(1, 23), end=(1, 24), line='# -*- coding: utf-8 -*-\n')
>>> next(tokens)
TokenInfo(type=1 (NAME), string='def', start=(2, 0), end=(2, 3), line='def hello_world():\n')
```

Here, we can see that open iterates over the lines of the file and generate_tokens iterates over them in a pipeline, doing additional work. Generators can also help in breaking the complexity and raising the efficiency of some data transformation algorithms that are based on several suites. Thinking of each suite as an iterator, and then combining them into a high-level function is a great way to avoid a big, ugly, and unreadable function. Moreover, this can provide a live feedback to the whole processing chain.  

In the following example, each function defines a transformation over a sequence. They are then chained and applied. Each function call processes one element and returns its result:  

在下面的例子中，每个函数定义一个

```python
def power(values):
    for value in values:
        print('powering %s' % value)
        yield value


def adder(values):
    for value in values:
        print('adding to %s' % value)
        if value % 2 == 0:
            yield value + 3
        else:
            yield value + 2
```

Here is the possible result of using these generators together:  

```shell
>>> elements = [1, 4, 7, 9, 12, 19]
>>> results = adder(power(elements))
>>> next(results)
powering 1
adding to 1
3
>>> next(results)
powering 4
adding to 4
7
>>> next(results)
powering 7
adding to 7
9
```

>### Tip 提示
>Keep the code simple, not the data 
>保持代码的简洁，而不是数据

>It is better to have a lot of simple iterable functions that work over sequences of values than a complex function that computes the result for entire collection at once.
>

Another important feature available in Python regarding generators is the ability to interact with the code called with the next function. yield becomes an expression, and a value can be passed along with a new method called send:  

Python中另外一个重要特征是生成器能够使用next函数和调用的代码进行交互。yield可以是一个表达式，

```python
def psychologist():
    print('Please tell me your problems')
    while True:
        answer = (yield)
        if answer is not None:
            if answer.endswith('?'):
                print("Don't ask yourself too much questions")
            elif 'good' in answer:
                print("Ahh that's good, go on")
            elif 'bad' in answer:
                print("Don't be so negative")
```

Here is an example session with our psychologist() function:  

这里是一个使用 psychologist()函数的例子：  

```
>>> free = psychologist()
>>> next(free)
Please tell me your problems
>>> free.send('I feel bad')
Don't be so negative
>>> free.send("Why I shouldn't ?")
Don't ask yourself too much questions
>>> free.send("ok then i should find what is good for me")
Ahh that's good, go on
```

send acts like next, but makes yield return the value passed to it inside of the function definition. The function can, therefore, change its behavior depending on the client code. Two other functions were added to complete this behavior—throw and close. They raise an error into the generator:  

send扮演者类似next的解决，而且让yield返回

- throw: This allows the client code to send any kind of exception to be raised.
- close: This acts in the same way, but raises a specific exception, GeneratorExit. In that case, the generator function must raise GeneratorExit again, or StopIteration.

- throw：

>### Note 注释
>Generators are the basis of other concepts available in Python—coroutines and asynchronous concurrency, which are covered in Chapter 13, Concurrency.

>生成器是在Python 协程和异步并发中使用的基本概念，其内容在第十三章－并发中提及。  

### Decorators 装饰器
Decorators were added in Python to make function and method wrapping (a function that receives a function and returns an enhanced one) easier to read and understand. The original use case was to be able to define the methods as class methods or static methods on the head of their definition. Without the decorator syntax, it would require a rather sparse and repetitive definition:  

在Python中装饰器的加入让函数和方法的包装（一个接受函数并返回一个增强版函数的函数）易于阅读和理解。原始的用例是能够定在定义方法之上将的类方法或者静态方法。不使用装饰器语法的话，就会要求些许重复的定义：   

```python
class WithoutDecorators:
    def some_static_method():
        print("this is static method")
    some_static_method = staticmethod(some_static_method)
    
    def some_class_method(cls):
        print("this is class method")
    some_class_method = classmethod(some_class_method)
```

If the decorator syntax is used for the same purpose, the code is shorter and easier to understand:   

如果装饰器语法被用于相同的目的，更短短代码更加易于理解：   

```python
class WithDecorators:
    @staticmethod
    def some_static_method():
        print("this is static method")
    
    @classmethod
    def some_class_method(cls):
        print("this is class method")
```

#### General syntax and possible implementations
The decorator is generally a named object (lambda expressions are not allowed) that accepts a single argument when called (it will be the decorated function) and returns another callable object. "Callable" is used here instead of "function" with premeditation. While decorators are often discussed in the scope of methods and functions, they are not limited to them. In fact, anything that is callable (any object that implements the __call__ method is considered callable), can be used as a decorator and often objects returned by them are not simple functions but more instances of more complex classes implementing their own __call__ method.  

The decorator syntax is simply only a syntactic sugar. Consider the following decorator usage:  

```python
@some_decorator
def decorated_function():
    pass
```

This can always be replaced by an explicit decorator call and function reassignment:  

```python
def decorated_function():
    pass

decorated_function = some_decorator(decorated_function)
```

However, the latter is less readable and also very hard to understand if multiple decorators are used on a single function.  

>#### Tip 提示
>Decorator does not even need to return a callable!
>装饰器甚至不需要返回可调用对象！

>As a matter of fact, any function can be used as a decorator because Python does not enforce the return type of decorators. So, using some function as a decorator that accepts a single argument but does not return callable, let's say str, is completely valid in terms of syntax. This will eventually fail if the user tries to call an object decorated this way. Anyway, this part of decorator syntax creates a field for some interesting experimentation.
>实际上，

##### As a function 用函数实现

There are many ways to write custom decorators, but the simplest way is to write a function that returns a subfunction that wraps the original function call.  

有很多编写自定义装饰器的例子，但是最简单的方式是编写一个能够返回包装了原始函数调用的子函数的函数。  

The generic patterns is as follows:  

通用模式如下：  

```python
def mydecorator(function):
    def wrapped(*args, **kwargs):     
        # do some stuff before the original
        # function gets called 在原始函数被调用之前要执行的动作
        result = function(*args, **kwargs)
        # do some stuff after function call and
        # return the result 在函数调用之后要做事情，并返回result
        return result
    # return wrapper as a decorated function 将包装器作为被装饰函数返回
    return wrapped
```

##### As a class 用类实现

While decorators almost always can be implemented using functions, there are some situations when using user-defined classes is a better option. This is often true when the decorator needs complex parametrization or it depends on a specific state.  

大多数的装饰器都可以使用函数实现，某些情况下使用自定义的类却是更好的选择。

The generic pattern for a nonparametrized decorator as a class is as follows:  

用类实现的非参数化装饰器通用模式如下：   

```python
class DecoratorAsClass:
    def __init__(self, function):
        def __call__(self, *args, **kwargs):
        # do some stuff before the original
        # function gets called 在原始函数被调用之前执行要执行的动作
        result = self.function(*args, **kwargs)
        # do some stuff after function call and
        # return the result
        return result
        self.function = function
```

##### Parametrizing decorators 带参数的装饰器

In real code, there is often a need to use decorators that can be parametrized. When the function is used as a decorator, then the solution is simple—a second level of wrapping has to be used. Here is a simple example of the decorator that repeats the execution of a decorated function the specified number of times every time it is called:   

在实际的代码中，经常需要把装饰器参数化。当函数当作装饰器时，那么解决方案就简单了——需要用到一个第二层包装。这里是一个在函数每次被调用时按照指定次数重复执行被装饰函数的装饰器的简单例子：   

```python
def repeat(number=3):
    """Cause decorated function to be repeated a number of times.
    Last value of original function call is returned as a result
    因为被装饰器函数将会重复多次。所以原始函数的最后一个值的调用作为result返回
    :param number: number of repetitions, 3 if not specified 
    ：参数 number：重复的次数，默认为3
    """
    def actual_decorator(function):
        def wrapper(*args, **kwargs):
            result = None
            for _ in range(number):
                result = function(*args, **kwargs)
            return result
        return wrapper
    return actual_decorator
```

The decorator defined this way can accept parameters:  

按照这种方式定义的装饰器可以接受参数：  

```
>>> @repeat(2)
... def foo():
...     print("foo")
...     
>>> foo()
foo
foo
```

Note that even if the parametrized decorator has default values for its arguments, the parentheses after its name is required. The correct way to use the preceding decorator with default arguments is as follows:  

注意，即使是参数化的装饰器对参数使用了默认值，这个装饰器的名称之后仍旧是需要使用圆括号的。如下为使用上面包含默认参数的装饰器的正确方式：  

```
>>> @repeat()
... def bar():
...     print("bar")
...     
>>> bar()
bar
bar
bar
```

Missing these parentheses will result in the following error when decorated function is called:  

当装饰的函数被调用时，缺失圆括号会出现如下错误结果：  

```
>>> @repeat
... def bar():
...     pass
...     
>>> bar()
Traceback (most recent call last):
  File "<input>", line 1, in <module>
TypeError: actual_decorator() missing 1 required positional
argument: 'function'
```

##### Introspection preserving decorators

Common pitfalls of using decorators is not preserving function metadata (mostly docstring and original name) when using decorators. All the previous examples have this issue. They created a new function by composition and returned a new object without any respect to the identity of the original one. This makes the debugging of functions decorated that way harder and will also break most of the auto-documentation tools that may be used because the original docstrings and function signatures are no longer accessible.  

使用装饰器的常见陷阱是在使用装饰器时没有保留函数元数据（多数为文档字符串和原始名称）。所有之前的例子都存在这个问题。

But let's see this in detail. Assume that we have some dummy decorator that does nothing more than decorating and some other functions decorated with it:   

但是，让我们来仔细看看。

```python
def dummy_decorator(function):
    def wrapped(*args, **kwargs):
        """Internal wrapped function documentation."""
        return function(*args, **kwargs)
    return wrapped


@dummy_decorator
def function_with_important_docstring():
    """This is important docstring we do not want to lose."""
```

If we inspect function_with_important_docstring() in a Python interactive session, we can notice that it has lost its original name and docstring:  

如果我们在Python交互式会话中检查function_with_important_docstring()，我们可以看到原始的名称和文档字符串丢失了：  

```
>>> function_with_important_docstring.__name__
'wrapped'
>>> function_with_important_docstring.__doc__
'Internal wrapped function documentation.'
```

A proper solution to this problem is to use the built-in wraps() decorator provided by the functools module:  

对于这个问题的正确解决方案是使用

```python
from functools import wraps


def preserving_decorator(function):
    @wraps(function)
    def wrapped(*args, **kwargs):
        """Internal wrapped function documentation."""
        return function(*args, **kwargs)
    return wrapped


@preserving_decorator
def function_with_important_docstring():
    """This is important docstring we do not want to lose."""
```

With the decorator defined in such a way, the important function metadata is preserved:  

```
>>> function_with_important_docstring.__name__
'function_with_important_docstring.'
>>> function_with_important_docstring.__doc__
'This is important docstring we do not want to lose.'
```

#### Usage and useful examples
Since decorators are loaded by the interpreter when the module is first read, their usage should be limited to wrappers that can be generically applied. If a decorator is tied to the method's class or to the function's signature it enhances, it should be refactored into a regular callable to avoid complexity. In any case, when the decorators are dealing with APIs, a good practice is to group them in a module that is easy to maintain.  

The common patterns for decorators are:  

- Argument checking
- Caching
- Proxy
- Context provider

##### Argument checking

Checking the arguments that a function receives or returns can be useful when it is executed in a specific context. For example, if a function is to be called through XML-RPC, Python will not be able to directly provide its full signature as in the statically-typed languages. This feature is needed to provide introspection capabilities, when the XML-RPC client asks for the function signatures.  

>#### Tip
>The XML-RPC protocol

>The XML-RPC protocol is a lightweight Remote Procedure Call protocol that uses XML over HTTP to encode its calls. It is often used instead of SOAP for simple client-server exchanges. Unlike SOAP, which provides a page that lists all callable functions (WSDL), XML-RPC does not have a directory of available functions. An extension of the protocol that allows discovering the server API was proposed, and Python's xmlrpc module implements it (refer to https://docs.python.org/3/library/xmlrpc.server.html).

A custom decorator can provide this type of signature. It can also make sure that what goes in and comes out respects the defined signature parameters:  

```python
rpc_info = {}


def xmlrpc(in_=(), out=(type(None),)):
    def _xmlrpc(function):
        # registering the signature
        func_name = function.__name__
        rpc_info[func_name] = (in_, out)
        def _check_types(elements, types):
            """Subfunction that checks the types."""
            if len(elements) != len(types):
                raise TypeError('argument count is wrong')
            typed = enumerate(zip(elements, types))
            for index, couple in typed:
                arg, of_the_right_type = couple
                if isinstance(arg, of_the_right_type):
                    continue
                raise TypeError(
                    'arg #%d should be %s' % (index, of_the_right_type))

        # wrapped function
        def __xmlrpc(*args):  # no keywords allowed
            # checking what goes in
            checkable_args = args[1:]  # removing self
            _check_types(checkable_args, in_)
            # running the function
            res = function(*args)
            # checking what goes out
            if not type(res) in (tuple, list):
                checkable_res = (res,)
            else:
                checkable_res = res
            _check_types(checkable_res, out)

            # the function and the type
            # checking succeeded
            return res
        return __xmlrpc
    return _xmlrpc
```

The decorator registers the function into a global dictionary and keeps a list of the types for its arguments and for the returned values. Note that the example was highly simplified to demonstrate argument-checking decorators.  

A usage example is as follows:  

```python
class RPCView:
    @xmlrpc((int, int))  # two int -> None
    def meth1(self, int1, int2):
        print('received %d and %d' % (int1, int2))

    @xmlrpc((str,), (int,))  # string -> int
    def meth2(self, phrase):
        print('received %s' % phrase)
        return 12
```

When it is read, this class definition populates the rpc_infos dictionary and can be used in a specific environment, where the argument types are checked:   

```
>>> rpc_info
{'meth2': ((<class 'str'>,), (<class 'int'>,)), 'meth1': ((<class 'int'>, <class 'int'>), (<class 'NoneType'>,))}
>>> my = RPCView()
>>> my.meth1(1, 2)
received 1 and 2
>>> my.meth2(2)
Traceback (most recent call last):
  File "<input>", line 1, in <module>
  File "<input>", line 26, in __xmlrpc
  File "<input>", line 20, in _check_types
TypeError: arg #0 should be <class 'str'>
```

##### Caching

The caching decorator is quite similar to argument checking, but focuses on those functions whose internal state does not affect the output. Each set of arguments can be linked to a unique result. This style of programming is the characteristic of functional programming (refer to http://en.wikipedia.org/wiki/Functional_programming) and can be used when the set of input values is finite.   

Therefore, a caching decorator can keep the output together with the arguments that were needed to compute it, and return it directly on subsequent calls. This behavior is called memoizing (refer to http://en.wikipedia.org/wiki/Memoizing) and is quite simple to implement as a decorator:   

```python
import time
import hashlib
import pickle

cache = {}


def is_obsolete(entry, duration):
    return time.time() - entry['time']> duration


def compute_key(function, args, kw):
    key = pickle.dumps((function.__name__, args, kw))
    return hashlib.sha1(key).hexdigest()


def memoize(duration=10):
    def _memoize(function):
        def __memoize(*args, **kw):
            key = compute_key(function, args, kw)

            # do we have it already ?
            if (key in cache and
                not is_obsolete(cache[key], duration)):
                print('we got a winner')
                return cache[key]['value']

            # computing
            result = function(*args, **kw)
            # storing the result
            cache[key] = {
                'value': result,
                'time': time.time()
            }
            return result
        return __memoize
    return _memoize
```

A SHA hash key is built using the ordered argument values, and the result is stored in a global dictionary. The hash is made using a pickle, which is a bit of a shortcut to freeze the state of all objects passed as arguments, ensuring that all arguments are good candidates. If a thread or a socket is used as an argument, for instance, a PicklingError will occur. (Refer to https://docs.python.org/3/library/pickle.html.) The duration parameter is used to invalidate the cached value when too much time has passed since the last function call.  

Here's an example of the usage:  

```
>>> @memoize()
... def very_very_very_complex_stuff(a, b):
...     # if your computer gets too hot on this calculation
...     # consider stopping it
...     return a + b
...
>>> very_very_very_complex_stuff(2, 2)
4
>>> very_very_very_complex_stuff(2, 2)
we got a winner
4
>>> @memoize(1) # invalidates the cache after 1 second
... def very_very_very_complex_stuff(a, b):
...     return a + b
...
>>> very_very_very_complex_stuff(2, 2)
4
>>> very_very_very_complex_stuff(2, 2)
we got a winner
4
>>> cache
{'c2727f43c6e39b3694649ee0883234cf': {'value': 4, 'time':
1199734132.7102251)}
>>> time.sleep(2)
>>> very_very_very_complex_stuff(2, 2)
4
```

Caching expensive functions can dramatically increase the overall performance of a program, but it has to be used with care. The cached value could also be tied to the function itself to manage its scope and life cycle, instead of a centralized dictionary. But in any case, a more efficient decorator would use a specialized cache library based on advanced caching algorithm.  

>#### Note
>Chapter 12, Optimization – Some Powerful Techniques, provides detailed information and techniques on caching.

##### Proxy

Proxy decorators are used to tag and register functions with a global mechanism. For instance, a security layer that protects the access of the code, depending on the current user, can be implemented using a centralized checker with an associated permission required by the callable:  

```python
class User(object):
    def __init__(self, roles):
        class Unauthorized(Exception):
    pass


def protect(role):
    def _protect(function):
        def __protect(*args, **kw):
            user = globals().get('user')
            if user is None or role not in user.roles:
                raise Unauthorized("I won't tell you")
            return function(*args, **kw)
        return __protect
    return _protect
```

This model is often used in Python web frameworks to define the security over publishable classes. For instance, Django provides decorators to secure function access.  

Here's an example, where the current user is kept in a global variable. The decorator checks his or her roles when the method is accessed:  

```
>>> tarek = User(('admin', 'user'))
>>> bill = User(('user',))
>>> class MySecrets(object):
...     @protect('admin')
...     def waffle_recipe(self):
...         print('use tons of butter!')
...
>>> these_are = MySecrets()
>>> user = tarek
>>> these_are.waffle_recipe()
use tons of butter!
>>> user = bill
>>> these_are.waffle_recipe()
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
File "<stdin>", line 7, in wrap
__main__.Unauthorized: I won't tell you
        self.roles = roles
```

##### Context provider

A context decorator makes sure that the function can run in the correct context, or run some code before and after the function. In other words, it sets and unsets a specific execution environment. For example, when a data item has to be shared among several threads, a lock has to be used to ensure that it is protected from multiple access. This lock can be coded in a decorator as follows:  

```python
from threading import RLock
lock = RLock()


def synchronized(function):
    def _synchronized(*args, **kw):
        lock.acquire()
        try:
            return function(*args, **kw)
        finally:
            lock.release()
    return _synchronized


@synchronized
def thread_safe():  # make sure it locks the resource
    pass
```

Context decorators are more often being replaced by the usage of the context managers (the with statement) that are also described later in this chapter.  

### Context managers – the with statement

The try...finally statement is useful to ensure some cleanup code is run even if an error is raised. There are many use cases for this, such as:  

- Closing a file
- Releasing a lock
- Making a temporary code patch
- Running protected code in a special environment

The with statement factors out these use cases by providing a simple way to wrap a block of code. This allows you to call some code before and after block execution even if this block raises an exception. For example, working with a file is usually done like this:   

```
>>> hosts = open('/etc/hosts')
>>> try:
...     for line in hosts:
...         if line.startswith('#'):
...             continue
...         print(line.strip())
... finally:
...     hosts.close()
...
127.0.0.1       localhost
255.255.255.255 broadcasthost
::1             localhost
```

>#### Note
>This example is specific to Linux since it reads the host file located in etc, but any text file could have been used here in the same way.

By using the with statement, it can be rewritten like this:  

```
>>> with open('/etc/hosts') as hosts:
...     for line in hosts:
...         if line.startswith('#'):
...             continue
...         print(line.strip )
...
127.0.0.1       localhost
255.255.255.255 broadcasthost
::1             localhost
```

In the preceding example, open used as a context manager ensures that the file will be closed after executing the for loop and even if some exception will occur.  

Some other items that are compatible with this statement are classes from the threading module:  

threading.Lock
threading.RLock
threading.Condition
threading.Semaphore
threading.BoundedSemaphore

#### General syntax and possible implementations
The general syntax for the with statement in the simplest form is:  

```python
with context_manager:
    # block of code
    ...
```

Additionally, if the context manager provides a context variable, it can be stored locally using the as clause:  

```python
with context_manager as context:
    # block of code
    ...
```

Note that multiple context managers can be used at once, as follows:  

```python
with A() as a, B() as b:
    ...
```

This is equivalent to nesting them, as follows:  

```python
with A() as a:
    with B() as b:
        ...
```

##### As a class

Any object that implements the context manager protocol can be used as a context manager. This protocol consists of two special methods:  

- `__enter__(self)`: More on this can be found at https://docs.python.org/3.3/reference/datamodel.html#object.__enter__
- `__exit__(self, exc_type, exc_value, traceback)`: More on this can be found at https://docs.python.org/3.3/reference/datamodel.html#object.__exit__

In short, the execution of the with statement proceeds as follows:  

1. The __enter__ method is invoked. Any return value is bound to target the specified as clause.
2. The inner block of code is executed. 
3. The __exit__ method is invoked.

__exit__ receives three arguments that are filled when an error occurs within the code block. If no error occurs, all three arguments are set to None. When an error occurs, __exit__ should not re-raise it, as this is the responsibility of the caller. It can prevent the exception being raised though, by returning True. This is provided to implement some specific use cases, such as the contextmanager decorator that we will see in the next section. But for most use cases, the right behavior for this method is to do some cleaning, like what would be done by the finally clause; no matter what happens in the block, it does not return anything.  

The following is an example of some context manager that implements this protocol to better illustrate how it works:  

```python
class ContextIllustration:
    def __enter__(self):
        print('entering context')

    def __exit__(self, exc_type, exc_value, traceback):
        print('leaving context')

        if exc_type is None:
            print('with no error')
        else:
            print('with an error (%s)' % exc_value)
```

When run without exceptions raised, the output is as follows:  

```
>>> with ContextIllustration():
...     print("inside")
...     
entering context
inside
leaving context
with no error
```

When the exception is raised, the output is as follows:  

```
>>> with ContextIllustration():
...     raise RuntimeError("raised within 'with'")
...     
entering context
leaving context
with an error (raised within 'with')
Traceback (most recent call last):
  File "<input>", line 2, in <module>
RuntimeError: raised within 'with'
```

##### As a function – the contextlib module

Using classes seems to be the most flexible way to implement any protocol provided in the Python language but may be too much boilerplate for many use cases. A contextlib module was added to the standard library to provide helpers to use with context managers. The most useful part of it is the contextmanager decorator. It allows you to provide both __enter__ and __exit__ parts in a single function, separated by a yield statement (note that this makes the function a generator). The previous example written with this decorator would look like the following code:  

```python
from contextlib import contextmanager

@contextmanager
def context_illustration():
    print('entering context')

    try:
        yield
    except Exception as e:
        print('leaving context')
        print('with an error (%s)' % e)
        # exception needs to be reraised
        raise
    else:
        print('leaving context')
        print('with no error')
```

If any exception occurs, the function needs to re-raise it in order to pass it along. Note that the context_illustration could have some arguments if needed, as long as they are provided in the call. This small helper simplifies the normal class-based context API exactly as generators do with the classed-based iterator API.  

The three other helpers provided by this module are:  

- closing(element): This returns the context manager that calls the element's close method on exit. This is useful for classes that deal with streams, for instance.
- supress(*exceptions): This suppresses any of the specified exceptions if they occur in the body of the with statement.
- redirect_stdout(new_target) and redirect_stderr(new_target): This redirects the sys.stdout or sys.stderr output of any code within the block to another file of the file-like object.

## Other syntax elements you may not know yet

There are some elements of the Python syntax that are not popular and rarely used. It is because they either provide very little gain or their usage is simply hard to memorize. Due to this, many Python programmers (even with years of experience) simply do not know about their existence. The most notable examples of such features are as follows:  

- The for … else clause
- Function annotations

### The for … else … statement

Using the else clause after the for loop allows you to execute a code of block only if the loop ended "naturally" without terminating with the break statement:   

```
>>> for number in range(1):
...     break
... else:
...     print("no break")
...
>>>
>>> for number in range(1):
...     pass
... else:
...     print("break")
...
break
```

This comes in handy in some situations because it helps to remove some "sentinel" variables that may be required if the user wants to store information if a break occurred. This makes the code cleaner but can confuse programmers not familiar with such syntax. Some say that such meaning of the else clause is counterintuitive, but here is an easy tip that helps you to remember how it works—memorize that else clause after the for loop simply means "no break".  

### Function annotations

Function annotation is one of the most unique features of Python 3. The official documentation states that annotations are completely optional metadata information about the types used by user-defined functions, but in fact, they are not restricted to type hinting, and also there is no single feature in Python and its standard library that leverages such annotations. This is why this feature is unique—it does not have any syntactic meaning. Annotations can simply be defined for a function and can be retrieved in runtime, but that is all. What to do with them is left to the developers.  

#### The general syntax
A slightly modified example from the Python documentation shows best how to define and retrieve function annotations:  

```
>>> def f(ham: str, eggs: str = 'eggs') -> str:
...     pass
...     
>>> print(f.__annotations__)
{'return': <class 'str'>, 'eggs': <class 'str'>, 'ham': <class 'str'>}
```

As presented, parameter annotations are defined by the expression evaluating to the value of the annotation preceded by a colon. Return annotations are defined by the expression between the colon denoting the end of the def statement and literal -> that follows the parameter list.  

Once defined, annotations are available in the __annotations__ attribute of the function object as a dictionary and can be retrieved during application runtime.  

The fact that any expression can be used as the annotation and it is located just near the default arguments allows to create some confusing function definitions as follows:  

```
>>> def square(number: 0<=3 and 1=0) -> (\
...     +9000): return number**2
>>> square(10)
100
```

However, such usage of annotations serves no other purpose than obfuscation and even without them it is relatively easy to write code that is hard to read and maintain.  

#### The possible uses
While annotations have a great potential, they are not widely used. An article explaining new features added to Python 3 (refer to https://docs.python.org/3/whatsnew/3.0.html) says that the intent of this feature was "to encourage experimentation through metaclasses, decorators, or frameworks". On the other hand, PEP 3107 that officially proposed function annotations lists the following set of possible use cases:  

- Providing typing information
    - Type checking
    - Let IDEs show what types a function expects and returns
    - Function overloading / generic functions
    - Foreign-language bridges
    - Adaptation
    - Predicate logic functions
    - Database query mapping
    - RPC parameter marshaling

- Other information
    - Documentation for parameters and return values

Although the function annotations are as old as Python 3, it is still very hard to find any popular and actively maintained package that uses them for something else than type checking. So function annotations are still mostly good only for experimentation and playing—the initial purpose why they were included in initial release of Python 3.  

## Summary

This chapter covered various best syntax practices that do not directly relate to Python classes and object-oriented programming. The first part of the chapter was dedicated to syntax features around Python sequences and collections, strings and byte-related sequences were also discussed. The rest of the chapter covered independent syntax elements of two groups—those that are relatively hard to understand for beginners (such as iterators, generators, and decorators) and those that are simply less known (the for…else clause and function annotations).  
